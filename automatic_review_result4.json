{
  "metadata": {
    "processing_time": 13.702875852584839,
    "timestamp": "2025-08-07T22:04:50.490224"
  },
  "reviews": [
    {
      "content": "This paper introduces ICAT, a novel evaluation framework designed to assess the factual accuracy and coverage of diverse factual information in long-form text generated by large language models (LLMs). ICAT operates through three modular implementations—ICAT-M, ICAT-S, and ICAT-A—that cater to varying assumptions about the availability of aspects and alignment methods. The framework involves the decomposition of generated texts into atomic claims, followed by the verification of these claims using external knowledge sources. ICAT is validated through human studies and is shown to exhibit strong correlation with human judgments. The framework's modular design facilitates its application across diverse domains, while its ability to assess both factuality and coverage represents a meaningful step forward in evaluating LLM outputs. However, the paper leaves several important areas, such as scalability, reproducibility, and methodological clarity, underexplored, limiting the framework’s broader applicability and interpretability.",
      "name": "Summary"
    },
    {
      "content": "ICAT addresses a critical gap in the evaluation of LLM-generated long-form texts by combining factuality and coverage into a cohesive framework. The modular design of ICAT enhances its adaptability to different domains and datasets, while its interpretable and fine-grained analyses provide valuable insights into the qualitative aspects of generated content. The paper is well-written and structured, making the framework easy to understand and implement. Notably, ICAT demonstrates strong correlation with human judgments, which is bolstered by a thorough human study. The inclusion of three distinct implementations (ICAT-M, ICAT-S, and ICAT-A) caters to varying evaluation scenarios, increasing the framework's usability. Furthermore, ICAT offers significant advancements over traditional metrics, such as ROUGE and BERTScore, by moving beyond surface-level text similarity to assess deeper semantic and factual properties. The study's emphasis on the simultaneous evaluation of factuality and coverage reflects a holistic approach that is particularly relevant for tasks requiring comprehensive, accurate information presentation.",
      "name": "Strengths"
    },
    {
      "content": "Despite its contributions, ICAT has several notable shortcomings. One key limitation is its dependence on external knowledge sources for verifying atomic claims, which raises concerns about scalability and generalizability, especially in real-world settings. The framework also relies on multiple heuristic decisions, such as setting thresholds for claim verification and determining the granularity of atomic claims, which may affect its reliability and fairness. Moreover, ICAT's evaluations lack sufficient depth in terms of interpretability; the paper does not provide adequate explanations for why specific claims are deemed accurate or inaccurate, nor does it sufficiently justify the design choices of its three implementations. Reproducibility is another concern, as the absence of code or detailed procedural descriptions hinders validation by other researchers. Furthermore, the experimental setup is somewhat outdated, relying on datasets and baselines that may not fully reflect current standards in the field. Finally, the paper fails to address potential biases in its use of the same LLM architecture for aspect generation and evaluation, which could undermine the objectivity of its assessments. These issues collectively limit ICAT's broader applicability and raise questions about its robustness and generalizability.",
      "name": "Weaknesses"
    },
    {
      "content": "Reject",
      "name": "Decision"
    },
    {
      "content": "You are an expert academic reviewer. Please read the following scientific paper and conduct a comprehensive and objective review of the paper.\n\n# Task Definition\nYou receive a scientific paper as the input. Please review this paper.\nIn your review, please point out the main contributions, strengths, and weaknesses of the paper, covering but not limited to the theory, methodology, innovation, significance, contribution, experimental design, dataset, logical reasoning, and writing expression of the paper.\nPlease provide fair and impartial review comments.\n\n# Return Format\nThe review should be organized into four sections:\n1. Summary: A overall summary of the paper.\n2. Strengths: Comments on the strengths of the paper.\n3. Weaknesses: Comments on the weaknesses of the paper.\n4. Decision: The final decision on the acceptance of the paper.\n\nContent of the paper to be reviewed:\nTitle: Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual Information in Long-form Text Generation\n\nAbstract:\n\nThis paper presents ICAT, an evaluation framework for measuring coverage of diverse factual information in long-form text generation. ICAT breaks down a long output text into a list of atomic claims and not only verifies each claim through retrieval from a (reliable) knowledge source, but also computes the alignment between the atomic factual claims and various aspects expected to be presented in the output. We study three implementations of the ICAT framework, each with a different assumption on the availability of aspects and alignment method. By adopting data from the diversification task in the TREC Web Track and the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong correlation with human judgments and provide comprehensive evaluation across multiple state-ofthe-art LLMs. Our framework further offers interpretable and fine-grained analysis of diversity and coverage. Its modular design allows for easy adaptation to different domains and datasets, making it a valuable tool for evaluating the qualitative aspects of long-form responses produced by LLMs. * Equal contribution. † Work done while visiting the Center for Intelligent Information Retrieval at UMass Amherst.\n\n\nMain Content:\n\n\n1 Introduction\nThe rapid advancement of Large Language Models (LLMs) has revolutionized long-form text generation, enabling increasingly sophisticated applications from report writing to complex question answering. However, this progress has highlighted a critical challenge: how do we effectively evaluate not just the factual accuracy of generated content, but also its completeness and coverage of diverse perspectives? While recent work has made significant progress in developing comprehensive evaluation frameworks for LLMs (Li et al., 2024), existing metrics often focus on isolated aspects like factuality or response quality (Min et al., 2023), failing to capture the multi-dimensional nature of high-quality long-form text.\nThe evaluation of long-form text generation presents unique challenges that go beyond traditional metrics (Samarinas et al., 2024). Lexical overlap metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) are fundamentally limited by their reliance on surface-level text similarity, making them inadequate for evaluating semantically equivalent but lexically different expressions. While more recent approaches like BERTScore (Zhang et al., 2019) and G-Eval (Liu et al., 2023) attempt to address this through semantic similarity, they still face fundamental limitations when applied to longform content. These metrics cannot effectively verify factual accuracy or assess whether the response comprehensively covers all relevant aspects of a topic. The vast space of possible acceptable outputs makes it impractical to create comprehensive reference texts, leading to the development of reference-free evaluation methodologies like Prism (Agrawal et al., 2021; Thompson and Post, 2020). However, these approaches often struggle to effectively identify hallucinations and biases.\nRecent approaches like FActScore (Min et al., 2023) and VERISCORE (Song et al., 2024) have addressed the factuality challenge by evaluating atomic claims against reliable sources, but factual accuracy alone is insufficient. A unified metric that considers both factuality and coverage is crucial not only for evaluation but also for optimizing LLM performance. Such a metric could serve as a reward function in reinforcement learning to simultaneously improve both the factual accuracy and comprehensive coverage of LLM outputs.\nConsider a user asking \"What are the health effects of coffee consumption?\" While an LLM response might present entirely factual claims about coffee's benefits, such as its role in improving alert-arXiv:2501.03545v3 [cs.CL] 17 Feb 2025 ness and potential protective effects against certain diseases, failing to address known health risks (like anxiety or sleep disruption) would present an incomplete and potentially misleading picture. This illustrates a critical gap in current evaluation approaches: the need to assess not just the accuracy of individual claims, but also the comprehensive coverage of diverse relevant information. This is particularly crucial for applications like medical information systems, policy analysis, text summarization, or educational content where balanced, complete information is essential for informed decision making.\nThis paper introduces ICAT, 1 a novel evaluation framework that addresses this gap by measuring both factual accuracy and coverage of diverse factual information in long-form text generation. Our key contributions include: 1) a modular evaluation framework that decomposes long-form text into atomic claims and evaluates both their factual accuracy and their alignment with expected aspects, 2) three implementations with varying degrees of automation, suitable for different evaluation scenarios, and 3) a comprehensive evaluation of various LLMs and demonstrating strong correlation with human judgments in a user study.\nICAT first breaks down the generated long text into atomic claims. Through retrieval from a (reliable) corpus C or the Web, ICAT verifies each atomic claim to ensure its factuality. To measure completeness and coverage of diverse facts, ICAT requires a set of diverse aspects to compute an alignment between each atomic factual claim in the LLM response and the set of diverse aspects. We study three implementations of the ICAT framework as follows: ICAT-M assumes that a groundtruth set of diverse claims are obtained manually and is available to the evaluation framework. It also assumes that the groundtruth relevance annotation for each document in the corpus C to each aspect is provided. Using this information, the retrieval model can identify which aspect is being covered by each atomic factual claim in the LLM response. ICAT-S similarly assumes that a groundtruth set of diverse claims are obtained manually, however no aspect-level relevance judgment is available. Therefore, it uses an LLM to conduct pseudo-labeling and performing alignment between the atomic factual claims and the set of aspects. On the other hand, ICAT-A assumes that the aspect set is not 1 source code: https://github.com/algoprog/ICAT available, so it first uses an LLM to automatically generate diverse aspects of the input and then conduct pseudo-labeling for alignment, as is done in the second variant.\nIn our experiments, we use ClueWeb (The Lemur Project, 2009) as the retrieval corpus. We solely focus on the English documents of the ClueWeb collection. In addition to custom open-source retrieval from ClueWeb, we explore web-based grounding using the Brave Search API. For experiments, we rely on the input queries from the TREC Web Track (Clarke et al., 2009 . The argument for this decision is based on the fact that TREC Web Track queries have also been used for search result diversification. This means that the queries include up to 7 aspects and documents are provided with aspect-level relevance annotations. Our experiments show that there is relatively strong corelation of ICAT with human judgments, showcasing the utility of this framework for evaluating coverage of diverse factual information in LLM responses without human input.(Clarke et al., , 2010\nBy offering a modular and adaptable framework, ICAT enables researchers to tailor the evaluation process to specific needs, making it a valuable tool for assessing the qualitative aspects of long-form responses produced by LLMs. The decomposition of LLM outputs into atomic claims and their alignment with specific topics makes the evaluation process highly interpretable -evaluators can trace exactly which claims support which topics and identify gaps in coverage. This granular analysis capability, combined with the framework's ability to evaluate both factual accuracy and topic coverage, provides a more comprehensive assessment compared to existing metrics that only measure one of these aspects.\n\n2 Related Work\nText Generation Evaluation Traditional approaches to evaluating generated text have primarily focused on n-gram overlap metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). While these metrics are effective for assessing local coherence and fluency, they fail to capture higher-level aspects such as topic coverage and diversity. Recent work has introduced more sophisticated metrics like BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020), and unified multi-dimensional evaluators (Zhong et al., 2022) which leverage pre- trained LLMs for more nuanced evaluation.\nTopic Coverage and Diversity Research on evaluating topic coverage has roots in information retrieval, where metrics like α-nDCG (Clarke et al., 2008) and S-Recall were used to assess the topical diversity of search results. The concept of diversity in evaluating generated text can encompass various interpretations, including lexical diversity (analyzing the variety of words used) and topical diversity (assessing the range of topics covered). In the context of text generation, recent work has explored various approaches to measuring lexical diversity, including term overlap self-similarity such as Self-BLEU (Zhu et al., 2018) and the proportion of distinct unigrams and bigrams in generated responses (Li et al., 2016). However, research on evaluation of topical diversity in LLMs is currently limited.\nLLM Evaluation Frameworks Several frameworks have been proposed for evaluating different aspects of LLM performance, including factuality (Min et al., 2023; Song et al., 2024) and dialogue quality (Mehri and Eskenazi, 2020). Concurrent to this research, the AutoNuggetizer framework (Pradeep et al., 2024) used LLMs to generate and assess the coverage of nuggets in text. The EXAM++ framework (Farzi and Dietz, 2024) evaluates information coverage by checking if system responses can answer query-related exam questions, focusing on high-level answerability. In contrast, our work provides more fine-grained explainability by aligning individual atomic claims with retrieved evidence to verify both factual accuracy and aspect coverage at a more granular level. This allows us to not only assess if an aspect is covered, but also identify the specific claims and evidence supporting that coverage.\nOur work builds upon these foundations, specifically addressing the challenge of evaluating topic coverage in long-form text generation while considering factuality at the same time.\n\n3 ICAT\nQueries that require a long-form response, e.g., complex non-factoid questions, are often associated with multiple aspects. The response to these queries often include multiple claims, some of which may be factually accurate, while others may be inaccurate. An ideal response to these queries should not only contain factually accurate claims, but should also leave no aspect or perspective unaddressed. For instance, an ideal answer to a question about a legislation should cover perspectives from all political parties. An ideal answer to a question about the impact of a food or a medication on health should cover both positive, neutral, and negative perspectives. However, no existing evaluation metric can evaluate both factual accuracy and aspect coverage in long-form text generation. To address these, given a long output y produced in response to an input x, the ICAT framework computes two main scores: factuality score and coverage score.\nFactuality Score. Building upon prior work, such as FActScore (Min et al., 2023) and VERISCORE (Song et al., 2024), Factuality Score measures the ratio (or percentage) of factually ac-Approach for Obtaining Diverse Aspects Approach for Claim-Aspect Alignment ICAT-M Manual: ground-truth aspects Manual: retrieval-based method with aspect-level ground-truth alignment ICAT-S Manual: ground-truth aspects Automatic: retrieval-based method with aspect-level LLM-based alignment ICAT-A Automatic: LLM-based aspect generation Automatic: retrieval-based method with aspect-level LLM-based alignment curate claims in y. To do so, it is crucial that the generated claims are accurate. Let AC(y) be a function that extracts atomic claims from a generated response y. Given the set of atomic claims C = AC(y) made in y, we define the function C T = CG(C; K) that verifies the factuality of claims in C using a given knowledge source K.\nTherefore, C T ⊆ C denotes the set of factually verified claims in response y. Factuality Score is then defined as follows:\nwhere | • | denotes the cardinality of the given set.\nCoverage Score. To evaluate information coverage and diversity in y, Coverage Score measures the ratio of aspects being covered by the factually accurate claims in the given text. Hence, it is essential to identify which query aspects are accurately addressed in the generated response. Formally, coverage score can be defined as:\nwhere T O is a function that identifies the subtopics associated with claim c, and T Q is a function that returns all aspects related to the input x. Note that aspect coverage is only computed for factually verified claims, i.e., C T , instead of all claims. The reason is that non-factual claims should be avoided, regardless of the aspect they cover. Thus, they should not contribute to the coverage score.\nThe ICAT β Score. Inspired by F-measure (Van Rijsbergen, 1979), we calculate the weighted harmonic average of these factuality and coverage scores, as follows:\nwhere parameter β is a hyper-parameter that controls the trade-off between the factuality and coverage scores. In more detail, β controls the weight of Coverage Score compared to Factuality Score. Thus, a higher β signifies the impact of information coverage, while a lower β prioritizes factual accuracy. The default value for β is equal to 1, where factuality and coverage score are weighted uniformly. Throughout this paper, when the value of β is not explicitly mentioned, the default value of 1 is being used.\nVariants of ICAT We study three variants of ICAT implementations based on how they obtain query aspects and compute the alignment between atomic claims and aspects. Table  describes the approaches used in these three variants and highlights their differences.\nThe rest of this section provides details on how to develop models for generating factual claims (i.e., function AC), how to validate the factuality of claims (i.e., function CG), and how to obtain query aspects and compute an alignment between factual claims and all query aspects (i.e., functions T Q and T O). The rest of this section describes the approaches at high level to introduce the generic ICAT, while Section 4 provides the implementation details used in our experiments.\n\n3.1 Atomic Claim Generation\nThe atomic claim generation process seeks to break down a given long text into standalone and atomic claim statements that preserve key context and maintain claim consistency (see Figure (Min et al., 2023)). The generated claims should strike an appropriate balance of granularity, ensuring they are self-contained and decontextualized. We assume the existence of a function C = AC(y), which returns a set of atomic claims C, given the long output text y. There are various ways to implement this; one might consider each sentence or paragraph in y as an atomic claim. However, this simple approach does not satisfy our expected self-containment and decontextualization qualities. Instead, we follow (Grattafiori et al., 2024) and utilize an LLM M claims with the prompt shown in Figure . This prompt instructs the LLM to decompose the generated response y into multiple self-explanatory and decontextualized sentences, each containing a single atomic fact. These sentences then constitute the set of atomic claims for the generated output, denoted as C = AC(y). An example of this process is illustrated in Figure . An instruction-tuned LLM can be used as M claims in a zero-of few-shot setting; however, we found that smaller-scale LLMs (such as LLaMA  with 8 billion parameters) cannot accurately perform this task without fine-tuning. Therefore, we either use an LLM with higher capacity or distill knowledge into a smaller-scale LLM, enabling faster inference for our framework. The details of this distillation process are provided in section 4.1.\n\n3.2 Claim Grounding\nTo design the claim grounding function CG, for each claim c ∈ C, we employ a retrieval model R to retrieve n documents from the given knowledge source K. Subsequently, a natural language inference (NLI) model M NLI is used to determine whether the claim can be supported by any of the retrieved documents. If the claim can be inferred from at least one of the retrieved documents, it is considered grounded (i.e., validated, thus factually accurate); otherwise, it is not. The function returns a subset of C that are found grounded.\n\n3.3 Aspect Coverage Assessment\nTo calculate aspect coverage for some given input prompt (query) x, it is essential to have a list of diverse aspects for x (i.e., T Q(x)) and a method to determine which aspect each claim pertains to (i.e., T O(c, K) : c ∈ C).\nMethods for Obtaining Diverse Query Aspects (T Q): We propose two main methods to identify all aspects related to the query x:\n• Manual-Ground-truth Aspects: In this case, the aspects that should be included in the re-sponse to the query x are provided as a reference for evaluation.\n• Automatic-LLM-based Aspect Generation: Building on previous work showing LLMs can effectively identify aspects of a query (Samarinas et al., 2022), we use an LLM M subtopic with the prompt shown in Figure . This prompt instructs the LLM to generate up to 10 aspects for the query, covering the key aspects about it. This approach is useful when ground-truth aspects are unavailable.\n\nMethods of Obtaining the Aspects of an Atomic Claim (T O):\nWe use two methods to identify the aspects related to an atomic claim c:\n• Manual-retrieval-based method with aspectlevel ground-truth alignment: In this method, we assume access to a knowledge source K, where each document is annotated with the aspects it covers. To find the aspects that the claim c covers, we use the retrieval model R to retrieve n documents. Then, according to the ranking, we find the first ranked document that supports the claim c using the method in Section 3.2. The aspects of this document are considered as the aspects that the claim c covers. If none of the documents support claim c, we assume that it does not cover any query aspect.\n• Automatic-retrieval-based method with aspect-level LLM-based alignment: In this method, we use an aspect-claim alignment LLM M coverage to determine which aspects each grounded claim covers. Given a query x, its aspects T Q(x), and a set of grounded claims C T , we prompt the LLM to analyze each claim and identify which aspects it addresses. The prompt (shown in Figure ) instructs the LLM to output a structured mapping between claims and aspects, where each claim can be mapped to zero, one, or multiple aspects. This approach eliminates the need for aspect-level relevance judgments in the knowledge source while still maintaining a retrieval-based verification of factual accuracy. Unlike the manual method that assumes a claim covers the aspects associated with its supporting document, this method directly analyzes the semantic relationship between claims and aspects, leading to more accurate assessment. Though ICAT-A shows weaker correlation than ICAT-S, we observed that auto-generated topics were more comprehensive and higher quality than the existing ones in the TREC dataset, suggesting ICAT-A would perform better with more complete ground-truth topics.\n4 Implementation Details\n\n4.1 Atomic Claim Generation\nThe claim generation module was trained with several key objectives in mind, building on recent work in atomic claim extraction (Min et al., 2023). The model learned to extract standalone factual statements from text while maintaining factual consistency and simplifying complex statements. Special attention was paid to preserving important context and qualifiers, and generating claims at an appropriate granularity level (Song et al., 2024). For this task we used Llama 3.1 8B (Grattafiori et al., 2024) fine-tuned using QLoRA (Dettmers et al., 2023) on synthetic examples. We tried using models of this size without fine-tuning, however we found that the generated claims are often not de-contextualized properly. Larger models with 70B or more parameters seem to be effective for this task without fine-tuning, however they are very expensive to run, especially for long texts. The synthetic training data was generated through a multi-stage process by prompting Llama 3.1 405B. We began by generating 200 diverse high-level topics across multiple domains. For each topic, we generated 5 relevant entities. We then created variable-length paragraphs for each entity and generated the associated list of atomic claims for each of them. Using these 1000 synthetic examples, we fine-tuned the model for 1 epoch with batch size 16, learning rate 2e-4 and LoRA parameters α = 16 and rank = 64.\n\n4.2 Topic Generation\nPrevious works have shown that LLMs can be very effective in query subtopic generation (Samarinas et al., 2022). In our framework, for generating ground-truth topics given a query, we use the same base LLM as the one used in claim generation. Here we found that even without fine-tuning, Llama 3.1 8B can produce relevant topics. In order to reduce the need for extra resources to use a base and fine-tuned version of the LLM for claim generation, we use the VLLM library (Kwon et al., 2023) to load the base model only once in memory and efficiently serve the adapter for the fine-tuned version.\n\n4.3 Claim Grounding\nWe implemented a two-stage approach for grounding atomic claims in the given text with a corpus. We first preprocess the corpus and generate chunks for each document with up to 128 words with 32 words overlap. We use a dense embedding model 2 (Merrick et al., 2024) to produce embeddings for all snippets and FAISS (Johnson et al., 2019) to build an efficient approximate nearest-neighbor index. We used IVF with HNSW for cluster assignment as our index type for fast search even when providing a large-scale corpus.\nIn the first stage, a retriever is used to obtain the k = 10 most relevant snippets in the corpus for each claim. When web search is used instead of a corpus, we use the returned snippets from Brave Search API. In the second stage, a natural language inference model is used to filter only the supported claims. We use a model based on DeBERTa V3 (He et al., 2021) fine-tuned on MultiNLI, FEVER and Adversarial NLI (Williams et al., 2018; Thorne et al., 2018; Nie et al., 2020). A claim is kept if there is at least one snippet that supports it. Instead of using a LLM for filtering the supported claims, we use a much smaller BERT based model (Devlin et al., 2019) fine-tuned specifically for this task.\nWe limit the snippet length because both NLI and dense embeddings models based on small pretrained transformer LMs like BERT tend to have lower performance as the input length increases.\n\n4.4 Aspect-Claim Alignment\nTopic coverage is assessed using the same base LLM with claim and topic generation. Given a query, a list of enumerated atomic claims and a list of ground truth topics, the LLM is prompted to produce a list of covered topic ids with their associated claim ids in structured jsonl format.\n\n5 Experimental Setup\nDataset. We conducted our experiments using the ClueWeb09 Category B corpus-a large-scale web collection with over 50 million English documents (The Lemur Project, 2009). This corpus has been used in TREC Web Track from 2009, consisting of 200 topics, derived from a commercial search engine's query log, balanced for popularity. Each topic includes a topic title (i.e., often used as the keyword search query), a description (i.e., detailed description of the information need), type, and subtopics (i.e., diverse aspects of the topic). The relevance judgments encompass 38,637 query-document pairs, with 19.06% (7366) marked as relevant. The dataset's unique advantage lies in its comprehensive coverage of internet content and human-annotated relevance judgments for topical diversity assessment. Relevance was judged either binary or on a five-point scale (later converted to binary), with documents considered relevant when containing useful information for specific subtopics. In our experiments with this collection, we filtered out spam documents using the Waterloo spam scorer to 2012 with the threshold of 70%. We used BM25 to retrieve 1000 documents for each topic (given its title as the query string) and considered these documents for retrieval in our factual verification process. The query set comprises 179 carefully selected faceted queries, each containing 3-8 subtopics representing different aspects of the information need.\nExperimental Setup. We evaluated four stateof-the-art LLMs: GPT-4, Llama-3-70B-Instruct, Mixtral-8x22B-Instruct-v0.1, and Openchat 3.5 (a fine-tune of Mistral-7B) (Wang et al., 2023). For each model, we generated responses for each test query. For the baselines in this paper, we used the query descriptions in their original format from the ClueWeb09 dataset as prompts, which are not optimized for producing very diverse outputs.\n\n6 Experimental Results\nHuman Evaluation Study. To validate ICAT's effectiveness, we conducted a comprehensive human evaluation study using Amazon Mechanical Turk (AMT). For each query-answer pair, three independent annotators assessed the coverage of aspects through a custom interface (Figure  in Appendix A.4). We limited the HITs to adult workers from the US, UK, Australia and Ireland, with over 98% approval rate who have completed at least 5,000 assignments. The annotators were tasked with identifying whether specific aspects are present in a given LLM-generated text and highlighting corresponding text evidence for each identified aspect. To ensure quality annotations, we provided detailed guidelines with two reference examples. We use majority voting across annotators. The study achieved substantial inter-annotator agreement with Fleiss's κ = 0.829, which is considered as a substantial agreement. For each query, we calculated Coverage Scores based on the set of covered topics identified by each evaluation method (ICAT variants) and by human annotators, relative to the set of ground truth topics. These per-query coverage scores were then used to compute linear and rank-bsaed correlation metrics (i.e., Pearson's ρ, Spearman's ρ, and Kendall's τ ) between the automated ICAT methods and human judgments.\nThe correlation analysis between ICAT variants and human judgments (see Table ) reveals strong performance across most evaluation methods. Using Llama-3.1-70B as the coverage model, ICAT-S achieves the strongest correlations (Pearson's ρ = 0.422, p < 0.01; Spearman's ρ = 0.446, p < 0.01). Interestingly, ICAT-A demonstrates better performance than ICAT-M despite not requiring groundtruth aspects. While ICAT-A seems to have much weaker correlation than ICAT-S, it is worth noting that in practice, we observed the automatically generated topics to be higher quality and more exhaustive. Using a more comprehensive set of groundtruth topics, ICAT-A would probably demonstrate higher correlation with human judgements.\nComparing factuality and coverage of information in state-of-the-art LLMs using ICAT.\nOur experimental results reveal distinct patterns in how different LLMs balance factuality and coverage (see Table ). When using corpus-based retrieval, Llama-3-70B demonstrates superior Coverage Score (0.451), while GPT-4 and Mixtral-8x22B show comparable factuality scores (0.343  and 0.344, respectively). However, Mixtral exhibits notably lower Coverage Score (0.370) compared to GPT-4's 0.416, resulting in lower overall ICAT 1 scores (0.297 vs 0.327). Notably, when employing web-based retrieval, we observe substantially higher factuality scores across all models. This significant improvement can be attributed to the broader knowledge base available through web search, allowing more claims to be successfully grounded. Coverage scores also show improvement with web-based retrieval, though the increase is more modest. This suggests that while web search enables better fact verification, the comprehensiveness of aspect coverage is more dependent on the model's capabilities than the retrieval source.\nControlling the trade-off between factuality and coverage using β.\n\n7 Conclusions and Future Work\nWe presented ICAT, a comprehensive framework for evaluating topic coverage in LLM-generated text. Through extensive experimentation using the ClueWeb09 dataset, we demonstrated the framework's effectiveness across different evaluation scenarios, with our best method achieving strong correlation with human judgments. The modular architecture of ICAT enables flexible adaptation to various evaluation requirements, from manual to automatic approaches for aspect identifications and alignment. Our results highlighted several key findings: (1) the importance of sophisticated coverage models in improving evaluation accuracy, (2) the viability of automatic evaluation approaches that maintain comparable performance to methods requiring ground truth annotations, and (3) the framework's ability to provide meaningful assessments across different LLM architectures and scales. In future work, individual components of our ICAT could be improved in terms of effectiveness and efficiency. Additionally, exploring the relationship between model size, evaluation accuracy, and computational efficiency could provide valuable insights for practical applications. Last but not least, the potential bias introduced by using the same or similar LLM when generating ground-truth aspects should be investigated. By using our metric, other works can explore methods for optimizing LLMs to produce more comprehensive outputs.\n\nLimitations\nOur evaluation framework, while showing promising results, suffers from several limitations that should be considered. First, our experiments reveal that even large language models with 70B parameters sometimes struggle with accurate aspect-claim alignment. This suggests that the correlation with human judgments could potentially be improved by specifically optimizing LLMs for this task, either through fine-tuning or more sophisticated prompting strategies. Second, our current implementation uses zeroshot prompting for query aspect generation without systematic evaluation of this component's effectiveness. Future work should explore methods to optimize and rigorously evaluate the aspect generation process, potentially through human evaluation or comparison with expert-curated aspect sets. This could lead to more reliable and comprehensive aspect coverage assessment.\nIn addition, there is a potential source of bias when using the same or similar LLM architecture both for generating query aspects and for producing responses for evaluation. This circular dependency might lead to artificially inflated performance metrics if the evaluated model shares similar biases or knowledge patterns with the model used for aspect generation. Future research should investigate the extent of this potential bias and explore methods to mitigate it, such as using diverse model architectures or more comprehensive human-curated aspects for evaluation.\nLast but not least, our current evaluation is limited to English-language content using web-based corpora. This narrow focus excludes evaluation of multilingual capabilities and limits the framework's applicability to other languages and cultures. Additionally, the reliance on web corpora may not be suitable for domains requiring specialized knowledge bases or authoritative sources. Future work should explore extending ICAT to support multilingual evaluation and integration with domain-specific knowledge bases to broaden its applicability across different languages, cultures, and specialized fields.\n\nA Appendix\n\nA.1 Atomic Claim Generation\nOur atomic claim generation process transforms complex text into atomic, verifiable statements while preserving essential context, as shown in Figure . The process focuses on creating decontextualized, self-contained claims that each express a single verifiable fact. When breaking down complex sentences, the process maintains important qualifiers, conditions, and temporal information while replacing contextual references with their explicit referents. For instance, a complex sentence about coffee's effects would be decomposed into separate claims about its components and their individual effects, with each claim being fully selfcontained and independently verifiable.\n\nA.2 Model Performance Analysis\nThe performance analysis presented in Tables  reveals several important patterns in model behavior. When comparing the 8B and 70B versions of Llama-3.1 as coverage models, we observe that the 8B model generally produces higher raw coverage scores, while the 70B model demonstrates stronger correlation with human judgments. This suggests a trade-off between computational efficiency and evaluation accuracy. The retrieval model comparison shows consistent advantages for dense retrieval methods over traditional BM25, with the Snowflake-Arctic-Embed models showing particular strength in handling queries where simple lexical matching is insufficient. Web-based retrieval consistently produces higher factuality scores compared to corpus-based approaches across all tested models. Table  analyzes the impact of different retrieval models on the correlation between ICAT variants and human judgments. The results show that larger dense retrieval models (Snowflake-Arctic-Embed-L) consistently outperform traditional BM25 across all ICAT variants, with improvements particularly notable in ICAT 2 and 3.\n\nA.3 Prompting Details\nThe framework employs three specialized prompts, each carefully designed for its specific task. The subtopic coverage assessment prompt (Figure ) requests the identification of covered subtopics from a given list, requiring evidence in the form of fact numbers that explicitly appear in the text. The prompt specifies a structured JSON output format where each line contains a topic ID and its supporting evidence. The atomic claim generation prompt (Figure ) focuses on extracting decontextualized, self-explanatory fact sentences from the input text, emphasizing the importance of resolved pronouns and independent context. The topic generation prompt (Figure ) elicits possible subtopics or related queries for a given query, requiring them to be ordered by importance and formatted as JSON objects, with a maximum of 10 topics.\n\nA.4 Human Evaluation Interface\nThe human evaluation interface shown in Figure  was developed to facilitate consistent and accurate topic coverage assessment. The interface presents the query and response in a split-screen layout, enabling annotators to highlight evidence for different topics using a color-coded system. To ensure annotation quality, we implemented minimum time requirements per response and included attention check questions. Annotators were guided to read responses completely before beginning their annotations and to identify text spans that provide direct evidence for topic coverage. Table : Comparative analysis of retrieval model impact on ICAT variants' correlation with human judgments, using Llama-3.1-70B for topic-claim alignment. Results demonstrate the superiority of dense retrieval models (Snowflake-Arctic-Embed) over traditional BM25 across all ICAT variants, with the largest improvements seen in ICAT-S and ICAT-A. The analysis includes three correlation metrics (Pearson's, Spearman's, and Kendall's) to provide a comprehensive view of alignment with human assessments across different statistical measures.  Figure : Topic generation prompt used to automatically generate diverse subtopics for a given query. The prompt requests possible subtopics or related queries ordered by importance, with output formatted as JSON objects and limited to a maximum of 10 topics. This automated approach enables reference-free evaluation when ground-truth topics are unavailable.\n\nReferences:\n\n[1] Assessing reference-free peer evaluation for machine translation\n[2] Meteor: An automatic metric for mt evaluation with improved correlation with human judgments\n[3] Overview of the trec 2009 web track\n[4] Overview of the trec 2010 web track\n[5] Overview of the trec 2011 web track\n[6] Overview of the trec 2012 web track\n[7] Novelty and diversity in information retrieval evaluation\n[8] Efficient and effective spam filtering and re-ranking for large web datasets\n[9] Qlora: Efficient finetuning of quantized llms\n[10] BERT: Pre-training of deep bidirectional transformers for language understanding\n[11] An exambased evaluation approach beyond traditional relevance judgments\n[12] The llama 3 herd of models\n[13] Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing\n[14] Billion-scale similarity search with GPUs\n[15] Efficient memory management for large language model serving with pagedattention\n[16] Llms-as-judges: A comprehensive survey on llm-based evaluation methods\n[17] A diversity-promoting objective function for neural conversation models\n[18] Rouge: A package for automatic evaluation of summaries\n[19] G-eval: NLG evaluation using gpt-4 with better human alignment\n[20] USR: An unsupervised and reference free evaluation metric for dialog generation\n[21] Arctic-embed: Scalable, efficient, and accurate text embedding models\n[22] FActScore: Fine-grained atomic evaluation of factual precision in long form text generation\n[23] Adversarial NLI: A new benchmark for natural language understanding\n[24] Bleu: a method for automatic evaluation of machine translation\n[25] Initial nugget evaluation results for the trec 2024 rag track with the autonuggetizer framework\n[26] Revisiting open domain query facet extraction and generation\n[27] Simulating task-oriented dialogues with state transition graphs and large language models\n[28] Bleurt: Learning robust metrics for text generation\n[29] VeriScore: Evaluating the factuality of verifiable claims in long-form text generation\n[30] The ClueWeb09 dataset. Accessed\n[31] Automatic machine translation evaluation in many languages via zero-shot paraphrasing\n[32] FEVER: a large-scale dataset for fact extraction and VERification\n[33] Information retrieval. 2nd. newton, ma\n[34] Openchat: Advancing open-source language models with mixed-quality data\n[35] A broad-coverage challenge corpus for sentence understanding through inference\n[36] Bertscore: Evaluating text generation with bert\n[37] Towards a unified multidimensional evaluator for text generation\n[38] Texygen: A benchmarking platform for text generation models\n</paper>",
      "isPromptInfo": true,
      "name": "PromptInfo"
    }
  ]
}
