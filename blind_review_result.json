{
  "processing_time": 110.87706065177917,
  "reviews": [
    {
      "review_id": "review_a",
      "sections": [
        {
          "content": "This paper introduces ICAT (Intersection of Coverage and Factuality), a novel framework designed to evaluate the factual accuracy and topical diversity of long-form text generation. The authors address a critical gap in the field, as existing metrics often fail to capture the nuances of human comprehension, particularly the balance between factual correctness and comprehensive coverage of diverse perspectives. The ICAT framework decomposes a generated text into atomic claims, which are then verified against a knowledge source using a retrieval model. The framework also computes a coverage score based on the alignment between atomic claims and relevant aspects derived from the query. ICAT offers three distinct variants: ICAT-M, which uses manually annotated aspects; ICAT-S, which uses a retrieval-based method with ground-truth topics for alignment; and ICAT-A, which employs an LLM to generate and align aspects without human annotation. The authors conducted experiments using the ClueWeb09 corpus and TREC Web Track queries, comparing the performance of ICAT against human judgments and other automatic evaluation methods. They also explored the impact of different components, such as retrieval models and claim grounding methods, on the overall evaluation scores. The results demonstrate that ICAT-S and ICAT-A achieve strong correlations with human judgments, while ICAT-M provides a more interpretable evaluation of both factuality and coverage. The authors further show that ICAT, particularly in its ICAT-S variant, can be used to evaluate the output of large language models (LLMs). This work is significant because it provides a flexible and modular framework that can be adapted to different scenarios, from manual annotation to fully automated evaluation. The ICAT framework addresses a crucial need for more comprehensive evaluation metrics that go beyond simple factuality scores and consider the relevance and diversity of generated content. The authors' approach of decomposing text into atomic claims and aligning them with query aspects offers a fine-grained analysis of text generation quality, allowing for a more nuanced understanding of both factual accuracy and topical diversity. The paper's findings suggest that ICAT has the potential to become a valuable tool for assessing the quality of long-form text generated by LLMs, contributing to the development of more reliable and informative AI systems. The modular design of ICAT also allows for future extensions and improvements, making it a versatile framework for ongoing research in text generation evaluation.",
          "name": "Summary"
        },
        {
          "content": "This paper presents several compelling strengths that contribute to its overall significance and potential impact. First, the research addresses a critical gap in the field of text generation evaluation by proposing a framework that simultaneously considers both factuality and coverage, which are often evaluated separately. This holistic approach is crucial for ensuring that generated content is not only accurate but also comprehensive and diverse. The modular and adaptable design of the ICAT framework is another significant strength. This design allows for easy integration with different domains and datasets, making it a versatile tool for a variety of applications. The three variants of ICAT, ICAT-M, ICAT-S, and ICAT-A, offer flexibility in terms of automation and reliance on manual annotations, catering to different evaluation scenarios. The empirical evaluation conducted in the paper is thorough and well-executed. The authors compare ICAT against human judgments and other automatic evaluation methods, demonstrating a strong correlation between ICAT scores and human assessments. This empirical validation provides strong evidence for the reliability and validity of the framework. The paper also explores the impact of different components, such as retrieval models and claim grounding methods, on the overall evaluation scores. This analysis provides valuable insights into the factors that influence the performance of ICAT and helps to guide future research in this area. The authors' focus on providing a more interpretable and fine-grained analysis of text generation quality is another notable strength. By decomposing text into atomic claims and aligning them with query aspects, ICAT offers a more detailed understanding of both factual accuracy and topical diversity. This fine-grained analysis is crucial for identifying specific areas of improvement in generated content. The paper is also well-written and clearly organized, making it accessible to a broad audience. The authors provide a comprehensive explanation of the ICAT framework, including the different variants and their implementation details. The inclusion of examples and visualizations further enhances the clarity of the presentation. Finally, the authors' exploration of using LLMs for aspect generation and alignment in the ICAT-A variant is a promising direction for future research. This approach has the potential to reduce the reliance on manual annotations and make ICAT more scalable for large-scale evaluations.",
          "name": "Strengths"
        },
        {
          "content": "While the paper presents a valuable framework, several weaknesses warrant careful consideration. A primary limitation lies in the lack of detailed implementation specifics for the atomic claim generation module. While the paper mentions using an LLM with instruction tuning, it omits crucial details such as the exact prompts used, the specific LLM models, and the fine-tuning parameters. For instance, the paper states, \"We used Llama 3.1 8B (Grattafiori et al., 2024) fine-tuned using QLoRA (Dettmers et al., 2023) on synthetic examples\" (Section 4.1). However, the prompts used to instruct the LLM to decompose the generated response y into multiple self-explanatory and decontextualized sentences are not provided. This lack of transparency makes it difficult to reproduce the results and assess the sensitivity of the framework to variations in this crucial step. This is a significant concern because the quality of the atomic claims directly impacts the subsequent evaluation steps. My confidence in this weakness is high, as the absence of these critical details is clearly evident in the paper. Another significant weakness is the lack of a thorough exploration of alternative retrieval methods beyond BM25. The paper states, \"We used BM25 to retrieve 1000 documents for each topic (given its title as the query string)\" (Section 4.3). However, BM25 is a relatively simple retrieval model, and the paper does not investigate the performance of more advanced techniques such as dense retrieval models, which have been shown to outperform BM25 in various information retrieval tasks. The paper mentions, \"We also use web-based grounding using the Brave Search API. For the baselines in this paper, we used the query descriptions in their original format from the ClueWeb09 dataset as prompts, which are not optimized for producing very diverse outputs.\" (Section 5), indicating that the baselines also use BM25. This omission limits the generalizability of the findings and raises questions about the robustness of the framework when using more advanced retrieval techniques. My confidence in this weakness is high, as the paper explicitly states the use of BM25 without exploring alternatives. The paper also lacks a detailed analysis of the performance of the claim grounding module, particularly in handling ambiguous or contradictory claims. The paper states, \"We used a dense embedding model M-density (Grattafiori et al., 2024) to produce embeddings for all snippets and FAISS (Johnson et al., 2019) to build an efficient approximate nearest-neighbor index. We used IVF with HNSW for cluster assignment as our index type for fast search even when providing a large-scale corpus.\" (Section 4.3). While the paper evaluates overall factuality and coverage scores, it does not provide specific metrics or examples to assess the robustness of the grounding process in handling such cases. This is a significant oversight, as ambiguity and contradiction are common in natural language and can lead to inaccurate evaluations. My confidence in this weakness is high, as the paper does not include a dedicated analysis of grounding accuracy for such cases. Furthermore, the paper's approach to aspect generation in ICAT-A relies on a single LLM, which can introduce bias and limit the comprehensiveness of the aspects. The paper states, \"In our framework, for generating ground-truth aspects given a query, we use the same base LLM as the one used in claim generation.\" (Section 3.3). While the paper mentions using Llama 3.1 8B for aspect generation, it does not explore the potential biases that might arise from this reliance on a single model. The paper also does not provide a detailed analysis of the trade-offs between using automated and human-annotated aspects, which could be a valuable avenue for future research. My confidence in this weakness is high, as the paper explicitly states the use of a single LLM for aspect generation. The paper also lacks a detailed discussion of the computational cost and scalability of the ICAT framework. While the paper provides the number of GPUs used for the experiments, it does not include a comprehensive analysis of the computational resources required, such as memory usage and processing time. This is a significant concern, especially for large-scale evaluations, where the computational cost can be a limiting factor. My confidence in this weakness is high, as the paper does not include a detailed analysis of the computational cost. Finally, the paper's baseline comparisons, while using state-of-the-art LLMs, are not as strong as they could be. The paper states, \"We evaluated four state-of-the-art LLMs: GPT-4, Llama-3-70B-Instruct, Mixtral-8x22B-Instruct-v0.1, and Openchat 3.5 (a fine-tune of Mistral-7B) (Wang et al., 2023).\" (Section 5). While these models are strong, the paper does not compare against methods like Ferret or Retrieval-Augmented FactScore, which have demonstrated strong performance in related tasks. This limits the assessment of ICAT's relative performance compared to the state-of-the-art. My confidence in this weakness is high, as the paper does not include comparisons against methods like Ferret or Retrieval-Augmented FactScore.",
          "name": "Weaknesses"
        },
        {
          "content": "Accept",
          "name": "Decision"
        }
      ]
    },
    {
      "review_id": "review_b",
      "sections": [
        {
          "content": "This paper introduces ICAT, a novel evaluation framework aimed at assessing both factual accuracy and topic coverage in long-form text generated by large language models (LLMs). The framework is built around modular components—atomic claim generation, claim grounding, and topic coverage assessment—that allow for flexible implementation ranging from fully manual to fully automated. Three variants of ICAT are proposed to cater to different evaluation settings. Experiments are conducted on the ClueWeb09 dataset, leveraging both human annotations and web-based searches for evaluation. The authors compare their approach to several baseline models and present empirical results, including correlations with human judgments. While the framework offers valuable contributions to text generation evaluation, the paper raises concerns regarding its applicability, generalizability, and reliance on LLM-generated components, as well as its limited exploration of alternative methods for atomic claim creation and grounding.",
          "name": "Summary"
        },
        {
          "content": "The paper makes a significant contribution to the field of long-form text evaluation by proposing ICAT, a framework that holistically addresses factual accuracy and topic coverage. The modular design of ICAT is particularly commendable, as it facilitates tailoring evaluations to specific scenarios and domains, enhancing its adaptability. The inclusion of multiple implementation variants—ICAT-M, ICAT-S, and ICAT-A—provides flexibility for both manual and automated assessments. The experimental results demonstrate strong correlations with human judgments, lending credibility to ICAT’s reliability. Furthermore, the use of diverse evaluation scenarios and state-of-the-art LLMs enhances the framework’s relevance and applicability. The paper is generally well-written, with clear articulation of its motivation and contributions. Its emphasis on both factual accuracy and topic coverage addresses a gap in existing evaluation metrics, thereby offering a more comprehensive assessment of long-form text generation. The inclusion of a human evaluation component adds another layer of validation to the framework's utility and reliability.",
          "name": "Strengths"
        },
        {
          "content": "Despite its merits, the paper has several limitations. The reliance on LLMs for components such as atomic claim generation and topic coverage assessment introduces potential biases, as the models used for these tasks are not thoroughly analyzed for their reliability or biases. This reliance may also undermine the objectivity of the evaluation process. Furthermore, the scope of experiments is limited to the ClueWeb09 dataset, which primarily consists of news articles, raising concerns about the framework's generalizability to other domains. The lack of comparisons with alternative methods for atomic claim creation and grounding, as well as the absence of ablation studies on key components such as retrieval models, is another notable shortcoming. The presentation of experimental results could also be improved, as the current tabular format lacks sufficient detail to fully understand the findings. Additional issues include the absence of a detailed discussion on computational efficiency, the limited number of baselines, and the weak correlation observed between ICAT-A and human judgments. These factors suggest the need for further refinement and expansion of the framework to enhance its applicability and robustness.",
          "name": "Weaknesses"
        },
        {
          "content": "Reject",
          "name": "Decision"
        }
      ]
    }
  ],
  "session_id": "1f21ab51-0008-45e5-a4e7-fbe35c980274"
}